"""
src/marag/indices/dense_index.py
Complete implementation with FAISS optimization
"""

import faiss
import numpy as np
import torch
from typing import List, Dict, Tuple, Optional
from sentence_transformers import SentenceTransformer
from dataclasses import dataclass
from pathlib import Path
import pickle
import logging

@dataclass
class DenseRetrievalResult:
    document_id: str
    score: float
    content: str
    metadata: Dict
    embedding: np.ndarray

class DenseVectorIndex:
    """Production-ready dense vector index with FAISS backend"""
    
    def __init__(self, 
                 embedder_name: str = "BAAI/bge-m3",
                 index_type: str = "IVFFlat",
                 embedding_dim: int = 1024,
                 use_gpu: bool = True):
        
        self.logger = logging.getLogger(__name__)
        self.embedder_name = embedder_name
        self.embedding_dim = embedding_dim
        
        # Initialize embedder with proper configuration
        self.embedder = SentenceTransformer(
            embedder_name,
            device="cuda" if torch.cuda.is_available() and use_gpu else "cpu"
        )
        
        # Configure FAISS index based on expected dataset size
        self.index = self._create_faiss_index(index_type)
        self.document_store = {}  # ID -> document mapping
        self.id_to_index = {}     # Document ID -> FAISS index mapping
        self.index_to_id = {}     # FAISS index -> Document ID mapping
        
        self.total_documents = 0
        self.is_trained = False
    
    def _create_faiss_index(self, index_type: str) -> faiss.Index:
        """Create appropriate FAISS index based on requirements"""
        if index_type == "IVFFlat":
            # For large datasets (>100K documents)
            quantizer = faiss.IndexFlatIP(self.embedding_dim)
            nlist = 100  # Number of clusters
            index = faiss.IndexIVFFlat(quantizer, self.embedding_dim, nlist, faiss.METRIC_INNER_PRODUCT)
            index.nprobe = 10  # Number of clusters to search
            return index
        elif index_type == "Flat":
            # For small to medium datasets
            return faiss.IndexFlatIP(self.embedding_dim)
        else:
            raise ValueError(f"Unsupported index type: {index_type}")
    
    def add_documents(self, 
                     documents: List[Dict], 
                     batch_size: int = 32,
                     show_progress: bool = True) -> None:
        """
        Add documents to the index with batch processing
        
        Args:
            documents: List of dicts with 'id', 'content', and 'metadata'
            batch_size: Batch size for embedding generation
            show_progress: Whether to show progress bar
        """
        if not documents:
            return
        
        self.logger.info(f"Adding {len(documents)} documents to dense index")
        
        # Extract texts for embedding
        texts = [doc["content"] for doc in documents]
        ids = [doc["id"] for doc in documents]
        
        # Generate embeddings in batches
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            batch_embeddings = self.embedder.encode(
                batch_texts,
                batch_size=batch_size,
                show_progress_bar=show_progress,
                normalize_embeddings=True,  # Crucial for cosine similarity
                convert_to_numpy=True
            )
            embeddings.append(batch_embeddings)
        
        embeddings = np.vstack(embeddings)
        
        # Store documents
        for idx, doc in enumerate(documents):
            doc_id = doc["id"]
            self.document_store[doc_id] = doc
            self.id_to_index[doc_id] = self.total_documents + idx
            self.index_to_id[self.total_documents + idx] = doc_id
        
        # Add to FAISS index
        if not self.is_trained and isinstance(self.index, faiss.IndexIVFFlat):
            # Train IVF index if needed
            self.logger.info("Training IVF quantizer...")
            self.index.train(embeddings)
            self.is_trained = True
        
        self.index.add(embeddings)
        self.total_documents += len(documents)
        
        self.logger.info(f"Added {len(documents)} documents. Total: {self.total_documents}")
    
    def search(self, 
               query: str, 
               top_k: int = 10,
               threshold: float = 0.0,
               return_embeddings: bool = False) -> List[DenseRetrievalResult]:
        """
        Search the index for similar documents
        
        Args:
            query: Search query string
            top_k: Number of results to return
            threshold: Minimum similarity score threshold
            return_embeddings: Whether to return embeddings in results
        
        Returns:
            List of retrieval results sorted by relevance
        """
        if self.total_documents == 0:
            return []
        
        # Encode query
        query_embedding = self.embedder.encode(
            [query],
            normalize_embeddings=True,
            convert_to_numpy=True
        )
        
        # Adjust top_k if needed
        actual_top_k = min(top_k, self.total_documents)
        
        # Search FAISS index
        scores, indices = self.index.search(query_embedding, actual_top_k)
        
        # Convert to results
        results = []
        for i in range(actual_top_k):
            idx = indices[0][i]
            score = float(scores[0][i])
            
            if score < threshold:
                continue
            
            doc_id = self.index_to_id.get(idx)
            if not doc_id:
                continue
            
            document = self.document_store[doc_id]
            
            result = DenseRetrievalResult(
                document_id=doc_id,
                score=score,
                content=document["content"],
                metadata=document.get("metadata", {}),
                embedding=None
            )
            
            if return_embeddings:
                # Store the embedding (would need to be saved separately)
                pass
            
            results.append(result)
        
        return sorted(results, key=lambda x: x.score, reverse=True)
    
    def fine_tune_embeddings(self, 
                           query_doc_pairs: List[Tuple[str, str]],
                           learning_rate: float = 1e-5,
                           epochs: int = 3,
                           batch_size: int = 16):
        """
        Fine-tune the embedding model on language-specific data
        
        Args:
            query_doc_pairs: List of (query, relevant_document) pairs
            learning_rate: Learning rate for fine-tuning
            epochs: Number of training epochs
            batch_size: Training batch size
        """
        self.logger.info(f"Fine-tuning embeddings on {len(query_doc_pairs)} pairs")
        
        # Implementation would use SentenceTransformer's fine-tuning capabilities
        # or custom contrastive learning
        
        # This is a simplified placeholder
        # Actual implementation would involve:
        # 1. Creating positive and negative pairs
        # 2. Setting up loss function (e.g., MultipleNegativesRankingLoss)
        # 3. Training loop
        # 4. Re-embedding all documents after fine-tuning
        
        raise NotImplementedError("Fine-tuning implementation required")
    
    def save(self, directory: Path) -> None:
        """Save index to disk"""
        directory.mkdir(parents=True, exist_ok=True)
        
        # Save FAISS index
        faiss.write_index(self.index, str(directory / "faiss_index.bin"))
        
        # Save metadata
        metadata = {
            "embedder_name": self.embedder_name,
            "embedding_dim": self.embedding_dim,
            "total_documents": self.total_documents,
            "is_trained": self.is_trained,
            "document_store": self.document_store,
            "id_to_index": self.id_to_index,
            "index_to_id": self.index_to_id
        }
        
        with open(directory / "metadata.pkl", "wb") as f:
            pickle.dump(metadata, f)
        
        self.logger.info(f"Saved index to {directory}")
    
    def load(self, directory: Path) -> None:
        """Load index from disk"""
        # Load FAISS index
        self.index = faiss.read_index(str(directory / "faiss_index.bin"))
        
        # Load metadata
        with open(directory / "metadata.pkl", "rb") as f:
            metadata = pickle.load(f)
        
        self.embedder_name = metadata["embedder_name"]
        self.embedding_dim = metadata["embedding_dim"]
        self.total_documents = metadata["total_documents"]
        self.is_trained = metadata["is_trained"]
        self.document_store = metadata["document_store"]
        self.id_to_index = metadata["id_to_index"]
        self.index_to_id = metadata["index_to_id"]
        
        # Reinitialize embedder
        self.embedder = SentenceTransformer(self.embedder_name)
        
        self.logger.info(f"Loaded index from {directory} with {self.total_documents} documents")
    
    def get_statistics(self) -> Dict:
        """Get index statistics"""
        return {
            "total_documents": self.total_documents,
            "embedding_dimension": self.embedding_dim,
            "embedder_model": self.embedder_name,
            "index_type": type(self.index).__name__,
            "is_trained": self.is_trained,
            "memory_usage_mb": self.index.ntotal * self.embedding_dim * 4 / (1024 * 1024)
        }
